# word_segmentation

This package provides several word segmenters, each implementing a different algorithm for segmenting a sentence into individual words.  Each segmenter takes as input a string which is a sentence with the spaces removed, and returns a list of words that make up the sentence.

A list of the segmenters and their algorithms are as follows (in implementation order, as each builds on the previous):
* **Max Match Word Segmenter**:  This class uses the maxmatch algorithm described in J&M (p 70 in 2E).  It starts at the beginning of the input, and greedily attempts to find the longest word in the dictionary that matches the sentence start.  It then continues to past the end of the found word to repeat the process until the entire input string is consumed.  This algorithm has bad results for English, but surprisingly, not the worst.  This class gets configured with a dictionary.
* **Greedy Unigram Word Segmenter**:  As with maxmatch, this algorithm starts at the beginning and greedily finds the best match, until the string is consumed.  However, in this case the best match is not the longest word, but the word with the highest frequency in the configured unigram provider.  The results for this algorithm are uniformly terrible, as can be expected.  This class and all the rest until the last get configured with a unigram provider.
* **Globally Optimizing Unigram Word Segmenter**:  This class uses dynamic programming to find the segmentation that will result in the highest total unigram count for all words in the segmentation.  To avoid having small, common words (such as the) overwhelm the entire segmentation, the logs of the frequencies were used instead of the actual frequencies (before that change, the results were pretty bad).  This algorithm still has the limitation that compound words will usually be split, as their components will almost always have higher frequencies than the word itself.
* **Real Word Maximizing Word Segmenter**:  Instead of maximizing unigram frequencies, this class's algorithm merely attempts to maximize the ratio of words in the dictionary to words not in the dictionary.  The results are average, or surprisingly good for how simple the algorithm sounds, though the algorithm and weightings did require some tweaking to work (which took away from its simplicity).
* **Globally Optimizing Word Length Maximizing Unigram Word Segmenter**: This algorithm attempted to work around the compound-word-splitting limitation of the Globally Optimizing Unigram Word Segmenter by rewarding longer words to a ridiculous extent -- the frequency for each word was multiplied by a large power of the length of the word.  This was one of the best algorithms, and the only one of the unigram algorithms to not split 'homework'.  (Thanks to Gregory Olmschenk for this suggestion.)
* **Bigram Word Segmenter**: This algorithm maximizes the probability of the bigrams that make up a segmentation, multiplying the probability by the number of words in the segmentation to make up for the additional fractional multiplications.  This algorithm has the best results overall, but when it fails, it fails horribly.  This class gets configured with both a unigram and a bigram provider.
